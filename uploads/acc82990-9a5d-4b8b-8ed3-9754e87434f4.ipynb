{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Main Pipeline Function\n\nThe main function that connects all components together.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def qa_to_images_pipeline(source_type=\"file\", file_path=\"Hampi_Architecture_QA.txt\", \n                         vector_store_path=\"./chroma_architecture_qa_db\", query=None,\n                         num_images=1, output_dir=\"hampi_images\", monument_filters=None,\n                         reference_dir=DEFAULT_REFERENCE_IMAGES_DIR, reference_count=5,\n                         device=\"cuda\"):\n    \"\"\"\n    Complete pipeline to convert architectural Q&A data to realistic images\n    using reference-based image generation\n    \n    Args:\n        source_type: 'file' or 'vector_store'\n        file_path: Path to the QA text file (if source_type is 'file')\n        vector_store_path: Path to the vector store (if source_type is 'vector_store')\n        query: Search query for the vector store (if source_type is 'vector_store')\n        num_images: Number of images to generate per Q&A pair\n        output_dir: Directory to save the generated images\n        monument_filters: List of specific monuments to include (e.g., ['Vitthala Temple'])\n        reference_dir: Directory containing reference images\n        reference_count: Number of reference images to use\n        device: Device to use for generation\n    \"\"\"\n    print(\"Loading models...\")\n    models = load_models(device=device)\n    \n    print(\"Retrieving Q&A pairs...\")\n    qa_pairs = get_qa_pairs(source_type, file_path, vector_store_path, query)\n    \n    if not qa_pairs:\n        print(\"No Q&A pairs found!\")\n        return []\n    \n    print(f\"Found {len(qa_pairs)} Q&A pairs.\")\n    \n    # Filter by monument if specified\n    if monument_filters:\n        filtered_pairs = []\n        for pair in qa_pairs:\n            if any(monument.lower() in pair[\"question\"].lower() for monument in monument_filters):\n                filtered_pairs.append(pair)\n        qa_pairs = filtered_pairs\n        print(f\"Filtered to {len(qa_pairs)} Q&A pairs related to specified monuments.\")\n    \n    generated_image_paths = []\n    \n    for i, pair in enumerate(qa_pairs):\n        print(f\"\\nProcessing Q&A pair {i+1}/{len(qa_pairs)}\")\n        print(f\"Question: {pair['question']}\")\n        \n        # Extract monument name from question if possible\n        monument_match = re.search(r'(Vitthala|Virupaksha|Krishna|Hazara Rama|Lotus Mahal|Elephant Stables)', pair['question'])\n        monument_name = monument_match.group(1) if monument_match else None\n        \n        # Extract architectural details\n        full_qa_text = f\"Question: {pair['question']}\\nAnswer: {pair['answer']}\"\n        architectural_details = extract_architectural_details(full_qa_text)\n        \n        if not architectural_details:\n            print(\"Could not extract architectural details, skipping...\")\n            continue\n        \n        print(f\"Extracted details: {architectural_details[:100]}...\")\n        \n        # Format image generation prompt\n        prompt, negative_prompt = format_image_prompt(architectural_details, monument_name)\n        print(f\"Generated prompt: {prompt[:100]}...\")\n        \n        # Generate images using reference-based approach\n        image_paths = generate_images(\n            prompt=prompt, \n            negative_prompt=negative_prompt, \n            models=models, \n            num_images=num_images, \n            output_dir=output_dir,\n            reference_dir=reference_dir,\n            reference_count=reference_count,\n            monument_name=monument_name\n        )\n        \n        generated_image_paths.extend(image_paths)\n    \n    print(f\"\\nGeneration complete. Created {len(generated_image_paths)} images in {output_dir}.\")\n    return generated_image_paths",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Example Usage\n\nBelow are examples of how to use the pipeline.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Example 1: Generate images from text file\n# generated_images = qa_to_images_pipeline(\n#     source_type=\"file\",\n#     file_path=\"Hampi_Architecture_QA.txt\",\n#     num_images=2,\n#     monument_filters=[\"Vitthala Temple\", \"pillars\"],\n#     output_dir=\"hampi_output\",\n#     reference_dir=DEFAULT_REFERENCE_IMAGES_DIR,\n#     reference_count=5\n# )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Example 2: Generate images from vector store with query\n# generated_images = qa_to_images_pipeline(\n#     source_type=\"vector_store\",\n#     vector_store_path=\"./chroma_architecture_qa_db\",\n#     query=\"What are the distinctive features of pillars in Hampi temples?\",\n#     num_images=2,\n#     output_dir=\"hampi_output\",\n#     reference_dir=DEFAULT_REFERENCE_IMAGES_DIR,\n#     reference_count=5\n# )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Display Generated Images\n\nUse this cell to display generated images.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Display images\nfrom IPython.display import Image, display\n\ndef display_generated_images(image_paths, max_display=3):\n    \"\"\"Display generated images in the notebook\"\"\"\n    if not image_paths:\n        print(\"No images to display.\")\n        return\n    \n    print(f\"Displaying {min(len(image_paths), max_display)} of {len(image_paths)} generated images:\")\n    \n    for i, path in enumerate(image_paths[:max_display]):\n        if os.path.exists(path):\n            print(f\"\\nImage {i+1}: {os.path.basename(path)}\")\n            display(Image(filename=path))\n        else:\n            print(f\"Image not found: {path}\")\n            \n    if len(image_paths) > max_display:\n        print(f\"\\n... and {len(image_paths) - max_display} more images\")\n\n# Uncomment to display images from a previous run\n# image_dir = \"hampi_output\"  # Directory containing generated images\n# image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) \n#                if f.endswith('.png') and 'comparison' not in f]\n# display_generated_images(image_files)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Direct Image Generation\n\nYou can also directly generate images from specific prompts without Q&A data.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def generate_direct_images(prompt_text, num_images=2, output_dir=OUTPUT_DIR, \n                          reference_dir=DEFAULT_REFERENCE_IMAGES_DIR, reference_count=5,\n                          device=\"cuda\"):\n    \"\"\"Generate images directly from a prompt without Q&A extraction\"\"\"\n    print(\"Loading models...\")\n    models = load_models(device=device)\n    \n    # Create a simple prompt based on the text\n    prompt, negative_prompt = format_image_prompt(prompt_text)\n    print(f\"Generated prompt: {prompt}\")\n    \n    # Generate images\n    image_paths = generate_images(\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        models=models,\n        num_images=num_images,\n        output_dir=output_dir,\n        reference_dir=reference_dir,\n        reference_count=reference_count\n    )\n    \n    return image_paths\n\n# Example: Direct generation\n# direct_images = generate_direct_images(\n#     prompt_text=\"Vittala temple with its famous stone chariot and musical pillars\",\n#     num_images=2\n# )\n# display_generated_images(direct_images)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "## Quick Generate with Reference Image\n\nThis utility lets you quickly generate images from a prompt using a specific reference image.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def quick_generate_with_reference(prompt_text, reference_image_path, output_dir=OUTPUT_DIR, \n                                 strength=0.75, guidance_scale=7.5, device=\"cuda\"):\n    \"\"\"Quickly generate an image based on a specific reference image\"\"\"\n    print(\"Loading models...\")\n    models = load_models(device=device)\n    \n    # Create prompt\n    prompt, negative_prompt = format_image_prompt(prompt_text)\n    \n    # Load and preprocess reference image\n    ref_image = Image.open(reference_image_path).convert(\"RGB\")\n    processed_ref = preprocess_reference_image(ref_image)\n    \n    # Generate seed\n    seed = random.randint(0, 2147483647)\n    \n    # Generate image\n    try:\n        print(f\"Generating from reference image: {os.path.basename(reference_image_path)}\")\n        generated_img = generate_from_reference(\n            models=models,\n            prompt=prompt,\n            reference_image=processed_ref,\n            negative_prompt=negative_prompt,\n            strength=strength,\n            guidance_scale=guidance_scale,\n            num_inference_steps=50,\n            seed=seed\n        )\n        \n        # Save image\n        os.makedirs(output_dir, exist_ok=True)\n        base_name = os.path.splitext(os.path.basename(reference_image_path))[0]\n        output_path = os.path.join(output_dir, f\"{base_name}_generated_seed{seed}.png\")\n        generated_img.save(output_path)\n        print(f\"Saved to: {output_path}\")\n        \n        # Save comparison\n        comparison = Image.new('RGB', (processed_ref.width * 2, processed_ref.height))\n        comparison.paste(processed_ref, (0, 0))\n        comparison.paste(generated_img, (processed_ref.width, 0))\n        comparison_path = os.path.join(output_dir, f\"{base_name}_comparison_seed{seed}.png\")\n        comparison.save(comparison_path)\n        \n        # Display images\n        print(\"Reference image:\")\n        display(Image(filename=reference_image_path))\n        print(\"Generated image:\")\n        display(Image(filename=output_path))\n        print(\"Side-by-side comparison:\")\n        display(Image(filename=comparison_path))\n        \n        return output_path, comparison_path\n    except Exception as e:\n        print(f\"Error generating image: {e}\")\n        return None, None\n\n# Example: Quick generation with a specific reference image\n# quick_generate_with_reference(\n#     prompt_text=\"The detailed ornate pillars of Hampi temples with intricate carvings\",\n#     reference_image_path=\"D:\\\\college\\\\imp-doc\\\\sem6\\\\GENAI\\\\project\\\\new\\\\all_images\\\\example.jpg\",\n#     strength=0.7  # 0.0 = keep reference exactly, 1.0 = completely replace\n# )",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}